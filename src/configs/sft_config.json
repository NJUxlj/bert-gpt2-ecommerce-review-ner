
{
    "model_path": "path/to/pretrained_model",
    "bert_model_name": "bert-base-chinese",
    "max_seq_length": 128,
    "learning_rate": 0.001,
    "batch_size": 10,
    "epochs": 3,
    "max_steps": 10000,
    "num_train_epochs": 2,
    "gradient_accumulation_steps": 2,
    "output_dir": "output",
    "lora_rank":32


}